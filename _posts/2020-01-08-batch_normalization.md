---
layout: post
title: Batch Normalization(BN)
subtitle: Deep Learning
category: Deep Learning
use_math: true
---

<br>

우선, 해당 포스트는 Stanford University School of Engineering의 [CS231n](https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=7) 강의자료와 [모두를 위한 딥러닝 시즌2](https://deeplearningzerotoall.github.io/season2/lec_pytorch.html)의 자료를 기본으로 하여 정리한 내용임을 밝힙니다.


<br>
<br>
### Vanishing Gradient

우리는 앞서 ReLU 등의 [변형된 Activation 함수를 사용]()하거나, Weight initialization를 신중하게 하는 방법을 통하여 Vanishing gradient 문제를 해결하는 ㅂ

<br>
<br>
### Batch Normalization

<br>

<center><img src = '/post_img/200108/image1.png' width="600"/></center>




<br>
<br>
### Reference

[CS231n](https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk), Stanford University School of Engineering

[모두를 위한 딥러닝 시즌2](https://deeplearningzerotoall.github.io/season2/lec_pytorch.html)
