---
layout: post
title: Ensemble
subtitle: Machine Learning
category: ML
use_math: true
---

이번 글은 연세대학교 김현중 교수님의 _Data Mining_ 강의노트의 내용을 참고한 내용임을 밝힙니다.

분류 문제를 해결할 수 있는 모델은 무수히 많이 존재한다.(Logistic regression, Random Forest 등...) 어떠한 Classifier $f$를 생각해보자.


<br>
<br>
# Variance in classification

일반적인 Classification 방법을 사용한다면 데이터셋이 달라짐에 따라 $f$는 아주 크게 변할 수 있다. ___Variance___ 는 Training dataset에 대한 Classifier $f$의 __민감도(sensitivity)__ 를 측정하는 척도로 볼 수 있다.

<br>
#### 1) Variance $\propto$ 1/N

일반적으로 모델의 Variance는 Training dataset의 크기 N과 반비례하는 성향을 보인다. N(Training dataset의 크기)이 커진다면 그만큼 모델의 Variance는 줄어들 것이다.

<br>
#### 2) Variance $\propto$ Complexity of model

반면, 모델의 Variance는 모델의 복잡한 정도(Complexity)와 비례하는 성향을 가진다. 즉, 일반적으로 모델이 복잡하거나 Overfitted될수록 Variance는 커진다.

<br>
<br>
# Bias in classification

#### Bias $\propto$ Simplicity of model

어떠한 모델 $f$를 고르더라도, 모델이 너무 간단하면 모델이 잘 data에 fitting되지 않아 bias가 증가할 수 있다.

<br>
<br>
# Stable vs Unstable

Low Variance, (상대적으로)High Bias의 모델을 우리는 __Stable__ 하다고 한다. 예를 들어, Logistic Regression 등이 있다.

High Variance, Low Bias의 모델을 우리는 __Unstable__ 하다고 한다. 예를 들어, Decision Tree, Neural Networks 등이 있다.

만약 모델에 __불안정성(Unstablity)__ 이 존재한다면 Ensemble을 통한 모델 성능 향상의 핵심 재료로 활용할 수 있다. Low Bias를 가지는 모델의 High Variance 문제만 해결할 수 있다면 기존의 모델보다 훨씬 더 좋은 모델을 만들 수 있기 때문이다.

<br>
<br>
# How?

<br>
### 1. Bagging

Bagging은 쉽게 생각해서 여러 모델들에 대하여 투표(voting)하는 것이라고 생각할 수 있다.

다음은 간단한 Bagging의 과정에 대한 설명이다.

---
---
우리는 J개의 Class를 예측할 수 있는 Classifier를 만들고자 한다.

<br>

1. Bootstrap을 통하여 B개의 training set을 sampling한다.(sample의 수가 N이라면, 복원추출로 N개 Sampling을 총 B번 한다.)

$$T^{(1)}, T^{(2)},  ..., T^{(B)}$$

<br>

2. 이렇게 만들어 낸 B개의 training set을 사용하여 B개의 Classifier를 만들어낸다.

$$C(x,T^{(1)}), C(x,T^{(2)}),...,C(x,T^{(B)})$$

<br>

3. 우리는 B개의 Classifier를 만들어냈다. 이 B개의 Classifier들은 어떠한 Input에 대하여 서로 다른 예측을 할 수 있다. B개의 Classification 결과, 우리는 _(총 J개의 Classes 중)_ j번째 class로 예측된 횟수를 $N_j$라고 할 수 있다.

$$ N_j = \sum_{b=1}^{B} I[C(x,T^{(b)})=j] \;\;\;\;for\;j=1,2,...,J$$

<br>

4. 이렇게 B번의 classification을 하였을 때 가장 많이 예측된 결과를 찾는다. 이를 최종 predicted class로 선택한다.

$$C_B(x) = argmax_j {(N_j)}$$

---
---





<br>
<br>
### 2. Boosting

<br>
### 3. stacking



<br>
<br>

### Reference
김현중(2018), Data Mining, 연세대학교

<br>
<x>
x
