---
layout: post
title: Tensor(1)
subtitle: Deep Learning
category: Deep Learning
use_math: true
---

<br>

우선, 해당 포스트는 Stanford University School of Engineering의 [CS231n](https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=7) 강의자료와 [모두를 위한 딥러닝 시즌2](https://deeplearningzerotoall.github.io/season2/lec_pytorch.html)의 자료를 기본으로 하여 정리한 내용임을 밝힙니다.

<br>
<br>
### Tensor

__Tensor__ 는 Pytorch, Tensorflow 등의 Open source 머신 러닝 라이브러리에서 사용하는 Matrix라고 볼 수 있다. Tensor에 대한 각종 연산은 Numpy와 매우 유사한데, 간단한 연산들을 위주로 차근차근 하나씩 살펴보도록 하겠다.

<br>
<br>
### Long & Float

기본적인 Python에서는 Int와 Float의 자료형을 사용하지만, (Py)Torch에서는 Long과 Float의 자료형을 사용한다.

```
import numpy as np
import torch

t_long = torch.LongTensor([1, 2, 3])
t_float = torch.FloatTensor([1, 2, 3])

print(t_long)
print(t_float)
```

이 때 t_long은 _tensor([1, 2, 3])_,

t_float은 _tensor([1., 2., 3.])_ 를 출력한다.


<br>
<br>
### 1D Tensor

```
### 1D Array with PyTorch
t = torch.FloatTensor([0, 1, 2, 3, 4, 5, 6])

print(t)
print(t.dim())      # rank
print(t.shape)      # shape
print(t.size())     # shape
print(t[1], t[-1])  # Element
```

t.dim()는 1을 출력하는데, 말 그대로 1차원 tensor이기 때문이다.

t.shape, t.size()는 Tensor의 shape을 나타내며, torch.Size([7])를 출력하게 된다.

Tensor는 numpy array와 같은 방법으로 slicing이 가능하다.

<br>
<br>
### 2D Tensor

```
### 2D Array with PyTorch
t = torch.FloatTensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9],
                       [10, 11, 12]
                      ])
print(t)
print(t.dim())  # rank
print(t.shape)
print(t.size()) # shape
print(t[:, 1])
```

t.dim()는 2차원 tensor이기 때문에 2를 출력한다.

t.shape, t.size()는 Tensor의 shape을 나타내며, 4 by 3 행렬로 볼 수 있으므로 torch.Size([4, 3])를 출력하게 된다.

역시 다차원 행렬 또한 slicing이 가능하다.

<br>
<br>
### Matrix Multiplication
```
### Matrix Multiplication
# matmul: Matrix Multiplication
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1.matmul(m2)) # 2 x 1

# mul, *: Elementwise Operation
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[5, 6], [7, 8]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 2
print(m1 * m2) # 2 x 2
print(m1.mul(m2))
```

일반적으로 우리가 알고 있는 행렬곱은 __A.matmul(B)__ 의 형태로 구할 수 있다.

__A*B__ 나, __A.mul(B)__ 는 행렬 곱이 아닌, elementwise operation이므로 주의하자.

<br>
<br>
### Mean
```
### Mean
t = torch.FloatTensor([1, 2])
print(t.mean())

t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)
print(t.mean())
print(t.mean(dim=0))
print(t.mean(dim=1))
print(t.mean(dim=-1))

t = torch.LongTensor([1, 2])
try:
    print(t.mean())
except Exception as exc:
    print(exc)
```

<br>

$$
\begin{align*}
\text{Tensor t}\\
\begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix}
\end{align*}
$$

t.mean()은 Tensor의 모든 성분에 대한 평균을 return해준다. 즉, (1+2+3+4)/4를 계산하여 _2.5_ 가 반환된다.

반면, dimension을 명시하여 mean function을 사용할 수도 있다. 2차원 tensor를 기준으로 __dim=0__ 의 경우 행의 방향으로 평균을 계산하라는 뜻이며, _tensor([2., 3.])_ 를 return해준다. 행의 방향으로 평균을 구하므로, (1+3)/2, (2+4)/2를 계산하기 때문이다.

마찬가지로 __dim=1__ 의 경우 열의 방향으로 평균을 계산하게 되며, _tensor([1.5000, 3.5000])_ 를 return해준다. 열의 방향으로 평균을 구하므로, (1+2)/2, (3+4)/2를 계산하기 때문이다.


<br>
<br>
### Max/Argmax
```
### Max, Argmax

t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)
print(t.max())      # Returns one value: max
print(t.max(dim=0)) # Returns two values: max and argmax
print('Max: ', t.max(dim=0)[0])
print('Argmax: ', t.max(dim=0)[1])
```

Pytorch에서 max() 함수는 기본적으로 max값과 argmax값을 함께 return해준다.

예를들어 t.max(dim=0)를 입력할 경우 __dim=0__ 에 의하여 행의 방향으로 max와 argmax를 구하게 되고, 1열과 2열에서 각각 3과 4가 max값이며, 해당 값들은 행 방향으로 index 1과 1에 위치한다. 따라서 다음과 같이 (max, argmax)를 return하게 된다.

_(tensor([3., 4.]), tensor([1, 1]))_


<br>
<br>
### Reference

[CS231n: Lecture 7, Training Neural Networks II](https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=7), Stanford University School of Engineering

[모두를 위한 딥러닝 시즌2](https://deeplearningzerotoall.github.io/season2/lec_pytorch.html)
