---
layout: post
title: Word2Vec
subtitle: Natural Language Processing(NLP)
category: Deep Learning 2
use_math: true
---

<br>
<br>
## Encoding

알고리즘이 언어를 이해할 수 있을까? 그렇다, 이해할 수 있다. 단, 어느정도의 전처리가 필요하다. 알고리즘이 이해하기에는 자연어는 너무나도 어려우며, 따라서 우리는 다음과 같이 숫자로 언어를 바꿔주는 과정을 거쳐 알고리즘에게 전달해준다. 다음의 간단한 문장을 살펴보자.

<br>

_love you. thank you._

위 문장에 등장하는 단어를 __Encoding__ 을 통하여 숫자로 바꿔주는 과정을 거치면 비로소 알고리즘도 이해할 수 있게 된다. 문제는 __어떻게__ Encoding할 것인지이다. 우선, 가장 간단하게 떠올릴 수 있는 방법은 Thank를 0, you를 1, love를 2로 encoding하는 방법이다.

$$
\begin{align*}
\text{thank} = 0\\
\text{you} = 1\\
\text{love} = 2\\
\end{align*}
$$

<br>

가장 많이 통용되는 방법은 다음과 같이 단어의 개수 만큼의 차원을 가지는 백터로 표시하는 __One Hot Encoding__ 을 사용하는 방법이다.

$$
\begin{align*}
\text{thank} = [1,0,0]\\
\text{you} = [0,1,0]\\
\text{love} = [0,0,1]\\
\end{align*}
$$

<br>

하지만 이 One Hot Encoding에는 한 가지 문제점이 존재한다. 바로 __Similarity(유사도)__ 가 없다는 점이다. 예를들어, 위 예시에서 'Thank'와 'love'는 아무래도 'you'보다는 유사하다고 보는 것이 타당하다. 하지만 이 문장에 나타나는 세 단어 벡터 간의 Euclidean distance는 $\sqrt{2}$로 같다. 또한, 세 벡터가 서로 이루는 각이 직각(90도)이기 때문에, Cosine similarity가 모두 0이되어 버린다.

<br>
<br>
## Embedding

다른 예시를 생각해보자. king, man, queen, woman이라는 4가지 단어가 있다고 가정하자. 이 네 단어를 One Hot Encoding을 통해 나타내면 다음과 같다.

$$
\begin{align*}
\text{king} = [1,0,0,0]\\
\text{man} = [0,1,0,0]\\
\text{queen} = [0,0,1,0]\\
\text{woman} = [0,0,0,1]\\
\end{align*}
$$

하지만 이 경우 역시 앞서 언급한 One Hot encoding의 고질적인 문제를 가지고 있다. 이러한 Encoding의 문제를 해결하기 위한 방법이 바로 __Embedding__ 이다. 다음은 __Embedding__ 의 예시이다.

<br>

$$
\begin{align*}
\text{king} = [1,2]\\
\text{man} = [1,3]\\
\text{queen} = [5,1]\\
\text{woman} = [5,3]\\
\end{align*}
$$

Embedding은 일반적으로 One Hot Encoding보다 저차원이며, Similarity를 갖는다는 특징을 가지고 있다. Embedding의 결과, 비슷한 의미를 가지는 단어들 간의 거리는 좁혀지고, 반대되는 의미를 가지는 단어들 간의 거리는 멀어지게 된다.

<br>
<br>
## Word2Vec

Word2Vec은 Embedding의 한 가지 방식이다. Word2Vect에는 크게 이를 설명하기 위하여 다음의 간단한 두 문장을 생각해보자.

_king brave man_

_queen beautiful woman_

작성 중...


<br>
<br>
### Reference

Test
