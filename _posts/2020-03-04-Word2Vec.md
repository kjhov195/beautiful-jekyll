---
layout: post
title: Word2Vec
subtitle: Natural Language Processing(NLP)
category: Deep Learning 2
use_math: true
---

<br>

우선, 해당 포스트는 [허민석, 딥러닝 자연어처리: Word2Vec](https://www.youtube.com/watch?v=sY4YyacSsLc&list=PLVNY1HnUlO24lnGmxdwTgfXkd4qhDbEkG&index=14)의 자료를 기본으로 하여 정리한 내용임을 밝힙니다.


<br>
<br>
### Encoding

알고리즘이 언어를 이해할 수 있을까? 그렇다, 이해할 수 있다. 단, 어느정도의 전처리가 필요하다. 알고리즘이 이해하기에는 자연어는 너무나도 어려우며, 따라서 우리는 다음과 같이 자연어를 숫자로 바꿔주는 과정을 거쳐 알고리즘에게 전달해준다. 다음의 간단한 문장을 살펴보자.

<br>

_love you. thank you._

위 문장에 등장하는 단어를 __Encoding__ 을 통하여 숫자로 바꿔주는 과정을 거치면 비로소 알고리즘도 이해할 수 있게 된다. 문제는 __어떻게__ Encoding할 것인지이다. 우선, 가장 간단하게 떠올릴 수 있는 방법은 Thank를 0, you를 1, love를 2로 encoding하는 방법이다.

$$
\begin{align*}
\text{thank} = 0\\
\text{you} = 1\\
\text{love} = 2\\
\end{align*}
$$

<br>

가장 많이 통용되는 방법은 다음과 같이 단어의 개수 만큼의 차원을 가지는 백터로 표시하는 __One Hot Encoding__ 을 사용하는 방법이다.

$$
\begin{align*}
\text{thank} = [1,0,0]\\
\text{you} = [0,1,0]\\
\text{love} = [0,0,1]\\
\end{align*}
$$

<br>

하지만 이 One Hot Encoding에는 한 가지 문제점이 존재한다. 바로 __Similarity(유사도)__ 가 없다는 점이다. 예를들어, 위 예시에서 'Thank'와 'love'는 아무래도 'you'보다는 유사하다고 보는 것이 타당하다. 하지만 이 문장에 나타나는 세 단어 벡터 간의 Euclidean distance는 $\sqrt{2}$로 같다. 또한, 세 벡터가 서로 이루는 각이 직각(90도)이기 때문에, Cosine similarity가 모두 0이되어 버린다.

<br>
<br>
### Embedding

다른 예시를 생각해보자. king, man, queen, woman이라는 4가지 단어가 있다고 가정하자. 이 네 단어를 One Hot Encoding을 통해 나타내면 다음과 같다.

$$
\begin{align*}
\text{king} = [1,0,0,0]\\
\text{man} = [0,1,0,0]\\
\text{queen} = [0,0,1,0]\\
\text{woman} = [0,0,0,1]\\
\end{align*}
$$

하지만 이 경우 역시 앞서 언급한 One Hot encoding의 고질적인 문제를 가지고 있다. 이러한 Encoding의 문제를 해결하기 위한 방법이 바로 __Embedding__ 이다. 다음은 __Embedding__ 의 예시이다.

<br>

$$
\begin{align*}
\text{king} = [1,2]\\
\text{man} = [1,3]\\
\text{queen} = [5,1]\\
\text{woman} = [5,3]\\
\end{align*}
$$

Embedding은 일반적으로 One Hot Encoding보다 저차원이며, Similarity를 갖는다는 특징을 가지고 있다. Embedding의 결과, 비슷한 의미를 가지는 단어들 간의 거리는 좁혀지고, 반대되는 의미를 가지는 단어들 간의 거리는 멀어지게 된다.

<br>
<br>
### Word2Vec

Word2Vec은 Embedding의 한 가지 방식이다. Word2Vec은 크게 CBOW(Continuous Bag of Words)와 Skip-Gram방식이 있다. 각각에 대하여 살펴보도록 하자.

<br>
<br>
### Word2Vec(CBOW)

CBOW의 경우, 주변 단어의 정보로부터 중심 단어의 정보를 유추하며, 여러개의 단어를 나열한 후에 그 단어들과 관련된 하나의 단어를 추정하는 문제라고 생각하면 된다. 예를들어 다음과 같은 문장을 생각해보자.

_Just like letters on the sand where waves were_

이 문장에서 ``(on, the, sand, where)`` 이라는 문맥이 주어진 경우, ``(waves)``를 예측하는 문제를 푸는 것이 Word2Vec(CBOW)의 목적이다.

<br>
<br>
### Word2Vec(Skip-Gram)

Skip-Gram 방식은 CBOW 방식과 반대로 특정한 단어로부터 문맥에서 등장할 수 있는 단어를 예측하는 문제를 풀고자 한다. Skip-Gram의 경우, input word 주변의 $k$ 개 단어를 문맥으로 보고 예측 모형을 만드는데 이 $k$를 window size라고 부른다.

_Just like letters on the sand where waves were_

예를들어 위 문장에서 window size가 $k=1$ 인 경우, input을 ``(letters)``로 주면 ``(like)``, ``(on)``를 예측하는 문제를 풀어야 하는 것이다.

또 다른 예시를 살펴보자.

_king brave man_

_queen beautiful woman_

window size가 1인 경우, 문장 안에서 해당 단어 주변의 한 단어만 살펴보게 되므로, 위 두 문장에서 얻을 수 있는 데이터는 다음과 같다.

|  <center>Word </center> |  <center>Neighbor</center> |  
|:--------|:--------:|--------:|
| <center> king </center> | <center> brave </center> |
| <center> brave </center> | <center> king </center> |
| <center> brave </center> | <center> man </center> |
| <center> man </center> | <center> brave </center> |
| <center> queen </center> | <center> beautiful </center> |
| <center> beautiful </center> | <center> queen </center> |
| <center> beautiful </center> | <center> woman </center> |
| <center> woman </center> | <center> beautiful </center> |

<br>

이번에는 window size가 2인 경우를 생각해보자. 문장 안에서 해당 단어 주변의 두 단어만 살펴보게 되므로, 위 두 문장에서 얻을 수 있는 데이터는 다음과 같다.

|  <center>Word </center> |  <center>Neighbor</center> |  
|:--------|:--------:|--------:|
| <center> king </center> | <center> brave </center> |
| <center> king </center> | <center> man </center> |
| <center> brave </center> | <center> king </center> |
| <center> brave </center> | <center> man </center> |
| <center> man </center> | <center> king </center> |
| <center> man </center> | <center> brave </center> |
| <center> queen </center> | <center> beautiful </center> |
| <center> queen </center> | <center> woman </center> |
| <center> beautiful </center> | <center> queen </center> |
| <center> beautiful </center> | <center> woman </center> |
| <center> woman </center> | <center> queen </center> |
| <center> woman </center> | <center> beautiful </center> |

<br>

window size가 2인 경우에 대하여 One Hot Encoding을 해주면 다음과 같이 나타낼 수 있다.

|  <center>Word </center> |  <center>Neighbor</center> |  
|:--------|:--------:|--------:|
| <center> [1,0,0,0,0,0] </center> | <center> [0,1,0,0,0,0] </center> |
| <center> [1,0,0,0,0,0] </center> | <center> [0,0,1,0,0,0] </center> |
| <center> [0,1,0,0,0,0] </center> | <center> [1,0,0,0,0,0] </center> |
| <center> [0,1,0,0,0,0] </center> | <center> [0,0,1,0,0,0] </center> |
| <center> [0,0,1,0,0,0] </center> | <center> [1,0,0,0,0,0] </center> |
| <center> [0,0,1,0,0,0] </center> | <center> [0,1,0,0,0,0] </center> |
| <center> [0,0,0,1,0,0] </center> | <center> [0,0,0,0,1,0] </center> |
| <center> [0,0,0,1,0,0] </center> | <center> [0,0,0,0,0,1] </center> |
| <center> [0,0,0,0,1,0] </center> | <center> [0,0,0,1,0,0] </center> |
| <center> [0,0,0,0,1,0] </center> | <center> [0,0,0,0,0,1] </center> |
| <center> [0,0,0,0,0,1] </center> | <center> [0,0,0,1,0,0] </center> |
| <center> [0,0,0,0,0,1] </center> | <center> [0,0,0,0,1,0] </center> |

이러한 One Hot Encoding된 matrix가 neural networks의 input으로 사용된다. 모델의 architecture를 살펴보면 다음과 같다.

<br>

<center><img src = '/post_img/200304/image1.png' width="600"/></center>

우리가 알고 있는 분류 문제를 위한 Neural Networks의 형태와 완벽하게 같다. 다만 유의할 점은, Embedding된 vector가 더 적은 차원의 vector가 될 수 있도록 weight matrix의 차원을 낮게 설정해주어야 한다는 것이다. 예를들어, 위 예시에서는 One Hot Encoding의 결과 6차원의 vector가 형성되었는데, 적어도 embedding하는 vector의 차원은 5차원 이하로 만들어주는 것이 좋다. 위 그림에서는 2차원의 vector로 만들어 준 것이다.


정리하면, 결국 우리가 얻은 One Hot Encoding 데이터에서 $ \text{Word} \rightarrow \text{Neighbor}$의 관계를 잘 설명해줄 수 있는 weight를 찾는 것이 목적이며, 이렇게 찾은 각 단어에 대한 weight 값이 각 단어에 대한 embedding 값이 된다.

<br>

<center><img src = '/post_img/200304/image2.png' width="600"/></center>

Word2Vec으로 Embedding된 값을 2차원 공간에 뿌려보면 위와 같이 나타나는 것을 확인할 수 있다.



<br>
<br>
### Reference

[허민석, 딥러닝 자연어처리: Word2Vec](https://www.youtube.com/watch?v=sY4YyacSsLc&list=PLVNY1HnUlO24lnGmxdwTgfXkd4qhDbEkG&index=14)
