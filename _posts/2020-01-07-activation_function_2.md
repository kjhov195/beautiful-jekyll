---
layout: post
title: Activation fucntion-tanh/ReLU/LeakyReLU/Maxout/ELU
subtitle: Deep Learning
category: Deep Learning
use_math: true
---

<br>

우선, 해당 포스트는 Stanford University School of Engineering의 [CS231n](https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=7) 강의자료와 [모두를 위한 딥러닝 시즌2](https://deeplearningzerotoall.github.io/season2/lec_pytorch.html)의 자료를 기본으로 하여 정리한 내용임을 밝힙니다.


<br>
<br>

### Sigmoid

세 가지 문제 중, 앞의 두 가지 문제 __Vanishing Gradient__ 와 __Not zero centered__ 는 큰 문제로 부각되지만, 계산상의 비효율성에 대한 문제는 사실 큰 문제가 되지는 않는다.



<br>
<br>
### tanh

<br>
<br>
### ReLU

<br>
<br>
### LeakyReLU

<br>
<br>
### Maxout


<br>
<br>
### ELU


<br>
<br>
### Example

<br>
<br>
### Reference

[CS231n](https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk), Stanford University School of Engineering

[모두를 위한 딥러닝 시즌2](https://deeplearningzerotoall.github.io/season2/lec_pytorch.html)
