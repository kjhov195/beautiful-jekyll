---
layout: post
title: Activation fucntion-Sigmoid/tanh/ReLU/LeakyReLU/Maxout/ELU
subtitle: Deep Learning
category: Deep Learning
use_math: true
---

<br>

우선, 해당 포스트는 Stanford University School of Engineering의 [CS231n](https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=7) 강의자료와 [모두를 위한 딥러닝 시즌2](https://deeplearningzerotoall.github.io/season2/lec_pytorch.html)의 자료를 기본으로 하여 정리한 내용임을 밝힙니다.


<br>
<br>
### Activation function


<br>
<br>
### Sigmoid

<br>
<br>
### tanh

<br>
<br>
### ReLU

<br>
<br>
### LeakyReLU

<br>
<br>
### Maxout


<br>
<br>
### ELU


<br>
<br>
### Example

<br>
<br>
### Reference

[CS231n](https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk), Stanford University School of Engineering

[모두를 위한 딥러닝 시즌2](https://deeplearningzerotoall.github.io/season2/lec_pytorch.html)

[PyTorch로 시작하는 딥 러닝 입문](https://wikidocs.net/60680)

[Example: Neural network](http://www.texample.net/tikz/examples/neural-network/)
