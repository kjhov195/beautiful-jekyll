---
layout: post
title: Optimization(2)-SGD/Nestrov/AdaGrad/RMSprop/Adam
subtitle: Deep Learning
category: Deep Learning
use_math: true
---

<br>

우선, 해당 포스트는
Stanford University School of Engineering의 [CS231n](https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=7)의 자료를 기본으로 하여 정리한 내용임을 밝힙니다.

<br>
<br>
### GD(Gradient Descent)

<br>
<br>
### SGD(Stochastic Gradient Descent)

<br>
<br>
### Nestrov optimizer

<br>
<br>
### AdaGrad optimizer

<br>
<br>
### RMS prop optimizer

<br>
<br>
### Adam optimizer


<br>
<br>
### Reference

[CS231n: Lecture 7, Training Neural Networks II](https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=7)
