---
layout: post
title: Boosting(1)-AdaBoost
subtitle: Machine Learning
category: Machine Learning
use_math: true
---


<br>
<br>

### 세 가지 특징

1. AdaBoost는 Classification을 위하여 (거의 항상) Stump, 즉 "Weak learners"들의 Combination을 활용한다.

2. 각 Stump들은 weight이 다르다.

3. 각 Stump는 직전 Stump에서의 error를 반영하여 만들어진다.


<br>
<br>

### 과정

---

[AdaBoost]

우리의 데이터셋에 데이터가 $N$개가 있으며, 2개의 class를 classification하고자 한다.

처음에는 데이터들에 대한 가중치가 각각 $1 \over N$일 것이다.($w_1=w_2=\cdots=w_N = {1 \over N}$)

<br>

(a) Dataset with $w_i(i=1,2,\cdots,N)$를 통해 Classifier $C_m(X)$를 학습시킨다.

(b) $err_m$을 다음과 같이 계산한다.

$$err_m = {\sum_{i=1}^N w_i I(y_i \neq C_m(x_i))  \over {\sum_{i=1}^N w_i}}$$

(c) $\alpha_m$ 을 다음과 같이 계산한다.

$$\alpha_m = log{(1-err_m) \over err_m}$$

(d) $w_i(i=1,2,\cdots,N)$을 다음과 같이 update한다.

$$w_{i,(update)} = w_{i,(previous)} \cdot \exp[\alpha_m \cdot I(y_i \neq C_m(x_i))] $$

<br>

그리고 (a)~(d) 과정을 반복한다.($m = 1,2, \cdots, M$)

최종 Output은 다음과 같다.

$$C_{Adaboost}(x) = sign[\sum_{m=1}^M \alpha_m C_m(x)]$$

---



<br>
<br>

### Reference
StatQuest with Josh Starmer(2019), [AdaBoost, Clearly Explained](https://www.youtube.com/watch?v=LsK-xG1cLYA)

<br>
<br>
